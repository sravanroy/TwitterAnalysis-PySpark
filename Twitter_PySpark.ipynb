{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"PySparkShell\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_schema = tp.StructType([\n",
    "  tp.StructField(name= 'id', dataType= tp.IntegerType(), nullable= True),\n",
    "  tp.StructField(name= 'label', dataType= tp.IntegerType(), nullable= True),\n",
    "  tp.StructField(name= 'tweet', dataType= tp.StringType(), nullable= True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Twitter Sentiments dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = spark.read.csv('twitter_sentiments.csv',\n",
    "                         schema=my_schema,\n",
    "                         header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "| id|label|               tweet|\n",
      "+---+-----+--------------------+\n",
      "|  1|    0| @user when a fat...|\n",
      "|  2|    0|@user @user thank...|\n",
      "|  3|    0|  bihday your maj...|\n",
      "|  4|    0|#model   i love u...|\n",
      "|  5|    0| factsguide: soci...|\n",
      "+---+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the machine learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stage 1: tokenize the tweet text    \n",
    "stage_1 = RegexTokenizer(inputCol= 'tweet' , outputCol= 'tokens', pattern= '\\\\W',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stage 2: remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stage 3: create a word vector of the size 100\n",
    "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stage 4: Logistic Regression Model\n",
    "model = LogisticRegression(featuresCol= 'vector', labelCol= 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the machine learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the pipeline\n",
    "pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, model])\n",
    "\n",
    "# fit the pipeline model with the training data\n",
    "pipelineFit = pipeline.fit(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream data and return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to compute sentiments of the received tweets\n",
    "def get_prediction(tweet_text):\n",
    "    try:\n",
    "    # filter the tweets whose length is greater than 0\n",
    "        tweet_text = tweet_text.filter(lambda x: len(x) > 0)\n",
    "    # create a dataframe with column name 'tweet' and each row will contain the tweet\n",
    "        rowRdd = tweet_text.map(lambda w: Row(tweet=w))\n",
    "    # create a spark dataframe\n",
    "        wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "    # transform the data using the pipeline and get the predicted sentiment\n",
    "        pipelineFit.transform(wordsDataFrame).select('tweet','prediction').show()\n",
    "    except : \n",
    "        print('No data')\n",
    "    \n",
    "# initialize the streaming context \n",
    "ssc = StreamingContext(sc, batchDuration= 3)\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:9991\n",
    "lines = ssc.socketTextStream(hostname='port',port= 9991)\n",
    "\n",
    "# split the tweet text by a keyword 'TWEET_APP' so that we can identify which set of words is from a single tweet\n",
    "words = lines.flatMap(lambda line : line.split('TWEET_APP'))\n",
    "\n",
    "# get the predicted sentiments for the tweets received\n",
    "words.foreachRDD(get_prediction)\n",
    "\n",
    "# Start the computation\n",
    "ssc.start()             \n",
    "\n",
    "# Wait for the computation to terminate\n",
    "ssc.awaitTermination()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
